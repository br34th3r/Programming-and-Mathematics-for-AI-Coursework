{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming and Mathematics for AI\n",
    "## Coursework\n",
    "\n",
    "This notebook contains all solutions annotated via markdown for the Programming and Mathematics for AI coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the whole project\n",
    "import math, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import mnist\n",
    "from collections import defaultdict\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "The second task is about classifying handwritten digits. We will use the MNIST dataset for training and testing.\n",
    "The point of this task is to develop a multi-layer neural network for classification using mostly Numpy:\n",
    "- Implement sigmoid and relu layers (with forward and backward pass)\n",
    "- Implement a softmax output layer\n",
    "- Implement a fully parameterizable neural network (number and types of layers, number of units)\n",
    "- Implement an optimizer (e.g. SGD or Adam) and a stopping criterion of your choosing\n",
    "- Train your Neural Network using backpropagation.\n",
    "\n",
    "Evaluate different neural network architectures and compare your different results. You can also compare with the results presented in http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the MNIST Dataset\n",
    "\n",
    "Below I am splitting the MNIST dataset into training and testing data, as well as taking a small sample of the dataset to display the type of data we will be working with for visual and debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "trainX = trainX.reshape(trainX.shape[0], trainX[0].shape[0] * trainX[0].shape[1])\n",
    "testX = testX.reshape(testX.shape[0], testX[0].shape[0] * testX[0].shape[1])\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1, Time Spent: 89.48s, Accuracy: 100.00%\n",
      "Epoch: 2, Time Spent: 171.68s, Accuracy: 100.00%\n",
      "Epoch: 3, Time Spent: 258.67s, Accuracy: 100.00%\n",
      "Epoch: 4, Time Spent: 347.39s, Accuracy: 100.00%\n",
      "Epoch: 5, Time Spent: 434.20s, Accuracy: 100.00%\n",
      "Epoch: 6, Time Spent: 520.95s, Accuracy: 100.00%\n",
      "Epoch: 7, Time Spent: 605.08s, Accuracy: 100.00%\n",
      "Epoch: 8, Time Spent: 688.42s, Accuracy: 100.00%\n",
      "Epoch: 9, Time Spent: 772.17s, Accuracy: 100.00%\n",
      "Epoch: 10, Time Spent: 854.35s, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "class MLNN:\n",
    "    def __init__(self, iLayerSize=784, hLayerSizes=(784, 784), oLayerSize=10, learningRate=0.001, epochs=10):\n",
    "        # Weights between hidden neurons and the output\n",
    "        # These do not refer to neuron values, but the weights between them\n",
    "        np.random.seed(100)\n",
    "        self.parameters = {}\n",
    "        self.learningRate = learningRate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.layers = [iLayerSize]\n",
    "        for hLayer in hLayerSizes:\n",
    "            self.layers.append(hLayer)\n",
    "        self.layers.append(oLayerSize)\n",
    "\n",
    "\n",
    "        # Every input neuron connects to every first layer hidden neuron\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.parameters[\"W\" + str(i)] = np.random.randn(self.layers[i - 1], self.layers[i]) * np.sqrt(1. / self.layers[i])\n",
    "            self.parameters[\"B\" + str(i)] = np.zeros((1, self.layers[i]))\n",
    "            \n",
    "\n",
    "    def forwardSigmoid(self, X):\n",
    "        params = self.parameters\n",
    "        params['A0'] = np.array(X)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            params['Z' + str(i)] = np.dot(params['A' + str(i - 1)], params['W' + str(i)]) + params['B' + str(i)]\n",
    "            if(i == len(self.layers) - 1):\n",
    "                self.parameters['A' + str(i)] = self.softmax(self.parameters['Z' + str(i)])\n",
    "            else:\n",
    "                self.parameters['A' + str(i)] = self.sigmoid(self.parameters['Z' + str(i)])\n",
    "        return params[\"A\" + str(len(self.layers) - 1)]\n",
    "    \n",
    "    def backwardsSigmoid(self, Y, output):\n",
    "        params = self.parameters\n",
    "        changes = {}\n",
    "        placeholder = self.parameters\n",
    "        for i in reversed(range(1, len(self.layers))):\n",
    "            if (i == len(self.layers) - 1):\n",
    "                error = 2 * (output - Y) / output.shape[0] * self.softmax(params['Z' + str(i)], derivative=True)\n",
    "                changes['W' + str(i)] = np.outer(error, params['A' + str(i - 1)])\n",
    "            else:\n",
    "                error = np.dot(error, params['W' + str(i + 1)].T) * self.sigmoid(params['Z' + str(i)], derivative=True)\n",
    "                changes['W' + str(i)] = np.outer(error, params[\"A\" + str(i - 1)])\n",
    "        return changes\n",
    "    \n",
    "    def backwardsRelu(self, Y, output):\n",
    "        params = self.parameters\n",
    "        changes = {}\n",
    "        placeholder = self.parameters\n",
    "        for i in reversed(range(1, len(self.layers))):\n",
    "            if (i == len(self.layers) - 1):\n",
    "                error = 2 * (output - Y) / output.shape[0] * self.softmax(params['Z' + str(i)], derivative=True)\n",
    "                changes['W' + str(i)] = np.outer(error, params['A' + str(i - 1)])\n",
    "            else:\n",
    "                error = np.dot(error, params['W' + str(i + 1)].T) * self.relu(params['Z' + str(i)], derivative=True)\n",
    "                changes['W' + str(i)] = np.outer(error, params[\"A\" + str(i - 1)])\n",
    "        return changes\n",
    "    \n",
    "    def updateWeights(self, changes):\n",
    "        for key, value in changes.items():\n",
    "            self.parameters[key] -= self.learningRate * np.transpose(value)\n",
    "    \n",
    "    # To be used with training data\n",
    "    def forwardRelu(self, X):\n",
    "        params = self.parameters\n",
    "        params['A0'] = np.array(X)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            params['Z' + str(i)] = np.dot(params['A' + str(i - 1)], params['W' + str(i)]) + params['B' + str(i)]\n",
    "            if(i == len(self.layers) - 1):\n",
    "                self.parameters['A' + str(i)] = self.softmax(self.parameters['Z' + str(i)])\n",
    "            else:\n",
    "                self.parameters['A' + str(i)] = self.relu(self.parameters['Z' + str(i)])\n",
    "        return params[\"A\" + str(len(self.layers) - 1)]\n",
    "    \n",
    "    def accuracy(self, x_val, y_val):\n",
    "        predictions = []\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.forwardSigmoid(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "        return np.mean(predictions)\n",
    "\n",
    "    def trainSigmoid(self, x_train, y_train, x_test, y_test):\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epochs):\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                output = self.forwardSigmoid(x)\n",
    "                changes = self.backwardsSigmoid(y, output)\n",
    "                self.updateWeights(changes)\n",
    "            accuracy = self.accuracy(x_test, y_test)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
    "                epoch+1, time.time() - start_time, accuracy * 100\n",
    "            ))\n",
    "    \n",
    "    def trainRelu(self, x_train, y_train, x_test, y_test):\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epochs):\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                output = self.forwardRelu(x)\n",
    "                changes = self.backwardsRelu(y, output)\n",
    "                self.updateWeights(changes)\n",
    "            accuracy = self.accuracy(x_test, y_test)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
    "                epoch+1, time.time() - start_time, accuracy * 100\n",
    "            ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x, derivative=False):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        if derivative:\n",
    "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x, derivative=False):\n",
    "        if derivative:\n",
    "            x[x<=0] = 0\n",
    "            x[x>0] = 1\n",
    "            return x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "mlnn = MLNN(iLayerSize=len(trainX[0]), hLayerSizes=(128, 64), oLayerSize=10)\n",
    "mlnn.trainRelu(trainX, trainY, testX[0], testY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('AI': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ed204f1bda4c8c45ed32d2146413eeb4d9b5a0e2c5624241a89b490b2018429"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}